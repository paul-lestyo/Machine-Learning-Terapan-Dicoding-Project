# -*- coding: utf-8 -*-
"""proyek_dicoding_ML_terapan2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17gDMYwfdeKs_sPDGNc4PGfTVhSvTiEF9

## Install package and get dataset

install package kaggle untuk download dataset
"""

!pip install -q kaggle

"""fungsi files untuk upload key kaggle"""

from google.colab import files
files.upload()

"""memindahkan kaggle.json agar dikenali platform"""

! mkdir ~/.kaggle
! cp kaggle.json ~/.kaggle/
! chmod 600 ~/.kaggle/kaggle.json

"""download dataset snehal1409/movielens dari kaggle kemudian di unzip"""

! kaggle datasets download -d snehal1409/movielens/
! mkdir data
! unzip movielens -d data

"""import kebutuhan library"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split

"""## Data Understanding

membaca dataset ratings
"""

rating = pd.read_csv("data/ratings.csv")
rating.info()

"""preview dataset rating"""

rating.head()

"""mengidentifikasi user yang melakukan rating dan banyak movie yang di rating"""

print("Banyak user yang melakukan rating:", rating.userId.nunique())
print("Banyak movie yang di rating:", rating.movieId.nunique())

"""memvisualkan sebaran data rating dari user"""

plt.figure(figsize=(10,5))
sns.countplot(x=rating.rating)

"""melihat dataset movies"""

movies = pd.read_csv("data/movies.csv")
movies.info()

"""preview dataset movies"""

movies.head()

"""mengidentifikasi total movies dan movie yang tidak di rating"""

print("Total judul movie:", movies.movieId.nunique())
print("Movie yang tidak di rating:", (movies.movieId.nunique() - rating.movieId.nunique()))

"""# Content-Based Filtering

## Data Preparation

memisahkan genres menjadi array
"""

movies["genres"] = movies["genres"].str.split("|")
movies

"""import TfidfVectorizer untuk representasi numerik"""

from sklearn.feature_extraction.text import TfidfVectorizer

"""menghitung bobot TF-IDF dari kolom genres"""

tf = TfidfVectorizer()
tf.fit(movies['genres'].apply(lambda x: ' '.join(x)))
tf.get_feature_names_out()

"""melihat shape matrix tfidf"""

tfidf_matrix = tf.fit_transform(movies['genres'].apply(lambda x: ' '.join(x)))
tfidf_matrix.shape

"""preview matrix tfidf"""

tfidf_matrix.todense()

"""preview sampel matrix tfidf berdasarkan genres"""

pd.DataFrame(
    tfidf_matrix.todense(),
    columns=tf.get_feature_names_out(),
    index=movies.movieId
).sample(22, axis=1).sample(10, axis=0)

"""menghitung cosine_similarity dari matrix tfidf"""

from sklearn.metrics.pairwise import cosine_similarity
cosine_sim = cosine_similarity(tfidf_matrix)
cosine_sim

"""preview sampel similarity movie satu dengan movie lainnya"""

cosine_sim_df = pd.DataFrame(cosine_sim, index=movies['movieId'], columns=movies['movieId'])
print('Shape:', cosine_sim_df.shape)

cosine_sim_df.sample(5, axis=1).sample(10, axis=0)

"""melihat bahwa movieId 8267 dan 3790 relevan (genre yang mirip)"""

filtered_movies = movies[(movies['movieId'] == 8267) | (movies['movieId'] == 3790)]
print(filtered_movies)

"""fungsi menampilkan movie rekomendasi sebanyak 5"""

def movie_recommendations(movieId, similarity_data=cosine_sim_df, items=movies[['movieId', 'title', 'genres']], k=5):

    # Mengambil data dengan menggunakan argpartition untuk melakukan partisi secara tidak langsung sepanjang sumbu yang diberikan
    # Dataframe diubah menjadi numpy
    # Range(start, stop, step)
    index = similarity_data.loc[:,movieId].to_numpy().argpartition(
        range(-1, -k, -1))

    # Mengambil data dengan similarity terbesar dari index yang ada
    closest = similarity_data.columns[index[-1:-(k+2):-1]]

    # Drop nama_resto agar nama resto yang dicari tidak muncul dalam daftar rekomendasi
    closest = closest.drop(movieId, errors='ignore')

    return pd.DataFrame(closest).merge(items).head(k)

"""mencari rekomendasi film yang relevan dengan movieId 1"""

movie_recommendations(1)

"""# Collaborative Filtering

import library yang dibutuhkan
"""

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

"""preview dataset rating"""

df = rating
df.head()

"""merge rating dengan title movie"""

df = pd.merge(df,movies[['movieId', 'title']],on='movieId')
df.head()

"""encoded / pemetaan nilai userId"""

user_ids = df['userId'].unique().tolist()
print('list userId: ', user_ids)

user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
print('encoded userID : ', user_to_user_encoded)

user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}
print('encoded angka ke userID: ', user_encoded_to_user)

"""encoded / pemetaan nilai movieId"""

movie_ids = df['movieId'].unique().tolist()

movie_to_movie_encoded = {x: i for i, x in enumerate(movie_ids)}

movie_encoded_to_movie = {i: x for i, x in enumerate(movie_ids)}

"""penggabungan nilai pemetaan user dan movie menjadi 1 bagian"""

df['user'] = df['userId'].map(user_to_user_encoded)
df['movie'] = df['movieId'].map(movie_to_movie_encoded)

"""melihat banyak user, banyak movie dan min max rating yang diberikan user"""

num_users = len(user_to_user_encoded)
print(num_users)

num_movie = len(movie_encoded_to_movie)
print(num_movie)

df['rating'] = df['rating'].values.astype(np.float32)

min_rating = min(df['rating'])
max_rating = max(df['rating'])

print('Number of User: {}, Number of movie: {}, Min Rating: {}, Max Rating: {}'.format(
    num_users, num_movie, min_rating, max_rating
))

"""menampilkan sampel dataframe setelah dipetakan nilai user dan movie"""

df = df.sample(frac=1, random_state=42)
df

"""split dataset training dan validation 8:2"""

x = df[['user', 'movie']].values

y = df['rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

train_indices = int(0.8 * df.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)

"""preview split data"""

print(x, y)

"""pembuatan model rekomendasi collaborative filtering dengan Keras.Model"""

class RecommenderNet(tf.keras.Model):

  # Insialisasi fungsi
  def __init__(self, num_users, num_movie, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_movie = num_movie
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding( # layer embedding user
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_users, 1) # layer embedding user bias
    self.movie_embedding = layers.Embedding( # layer embeddings movie
        num_movie,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.movie_bias = layers.Embedding(num_movie, 1) # layer embedding movie bias

  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0]) # memanggil layer embedding 1
    user_bias = self.user_bias(inputs[:, 0]) # memanggil layer embedding 2
    movie_vector = self.movie_embedding(inputs[:, 1]) # memanggil layer embedding 3
    movie_bias = self.movie_bias(inputs[:, 1]) # memanggil layer embedding 4

    dot_user_movie = tf.tensordot(user_vector, movie_vector, 2)

    x = dot_user_movie + user_bias + movie_bias

    return tf.nn.sigmoid(x) # activation sigmoid

"""konfigurasi model loss, optimizer dan metrics"""

model = RecommenderNet(num_users, num_movie, 50) # inisialisasi model

# model compile
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

"""training model dengan epochs 10x"""

history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 8,
    epochs = 10,
    validation_data = (x_val, y_val)
)

"""melakukan visualisasi proses training model"""

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""membuat sampel user untuk diberikan rekomendasi film"""

movie_df = movies
df = pd.read_csv('data/ratings.csv')

# Mengambil sample user
user_id = df.userId.sample(1).iloc[0]
movie_watched_by_user = df[df.userId == user_id]

movie_not_watched = movie_df[~movie_df['movieId'].isin(movie_watched_by_user.movieId.values)]['movieId']
movie_not_watched = list(
    set(movie_not_watched)
    .intersection(set(movie_to_movie_encoded.keys()))
)

movie_not_watched = [[movie_to_movie_encoded.get(x)] for x in movie_not_watched]
user_encoder = user_to_user_encoded.get(user_id)
user_movie_array = np.hstack(
    ([[user_encoder]] * len(movie_not_watched), movie_not_watched)
)

"""melakukan pemberian rekomendasi film oleh user berdasarkan pola prediksi dari model"""

ratings = model.predict(user_movie_array).flatten()

top_ratings_indices = ratings.argsort()[-10:][::-1]
recommended_movie_ids = [
    movie_encoded_to_movie.get(movie_not_watched[x][0]) for x in top_ratings_indices
]

print('Showing recommendations for users: {}'.format(user_id))
print('===' * 9)
print('movie with high ratings from user')
print('----' * 8)

top_movie_user = (
    movie_watched_by_user.sort_values(
        by = 'rating',
        ascending=False
    )
    .head(5)
    .movieId.values
)

movie_df_rows = movie_df[movie_df['movieId'].isin(top_movie_user)]
for row in movie_df_rows.itertuples():
    print(row.title, ':', row.genres)

print('----' * 8)
print('Top 10 movie recommendation')
print('----' * 8)

recommended_movie = movie_df[movie_df['movieId'].isin(recommended_movie_ids)]
for row in recommended_movie.itertuples():
    print(row.title, ':', row.genres)